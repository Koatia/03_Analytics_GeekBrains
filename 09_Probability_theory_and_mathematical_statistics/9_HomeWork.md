### Урок 9. Линейная регрессия Логистическая регрессия

#### Задание 1

_Даны значения величины заработной платы заемщиков банка (zp) и значения их поведенческого кредитного скоринга (ks). Используя математические операции, посчитать коэффициенты линейной регрессии, приняв за X заработную плату (то есть, zp - признак), а за y - значения скорингового балла (то есть, ks - целевая переменная). Произвести расчет как с использованием intercept, так и без._
> zp = [35, 45, 190, 200, 40, 70, 54, 150, 120, 110]
>
> ks = [401, 574, 874, 919, 459, 739, 653, 902, 746, 832]

Решение:

Для вычисления коэффициентов линейной регрессии вручную, будем использовать следующие формулы:

1. Коэффициент наклона (slope):

> $\quad \beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$

2. Свободный член (intercept):

> $\quad \beta_0 = \bar{y} - \beta_1 \cdot \bar{x}$

где $\bar{x}$ и $\bar{y}$ - средние значения $x$ и $y$ соответственно.

1. Вычисляем средние значения $\bar{x}$ и $\bar{y}$:

> $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i = \frac{35 + 45 + 190 + 200 + 40 + 70 + 54 + 150 + 120 + 110}{10} = \frac{1014}{10} = 101.4$

> $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i = \frac{401 + 574 + 874 + 919 + 459 + 739 + 653 + 902 + 746 + 832}{10} = \frac{7099}{10} = 709.9$

2. Вычисляем Коэффициент наклона (slope) $\beta_1$:

> $\beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$

> $\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) = (35-101.4)(401-709.9) + (45-101.4)(574-709.9) + \ldots + (110-101.4)(832-709.9) = 92917.28$
>
> $\sum_{i=1}^{n} (x_i - \bar{x})^2 = (35-101.4)^2 + (45-101.4)^2 + \ldots + (110-101.4)^2 = 34714.6$
>
> $\beta_1 = \frac{92917.28}{34714.6} \approx 2.677$

3. Вычисляем Свободный член (intercept) $\beta_0$:

> $\beta_0 = 709.9 - 2.677 \cdot 101.4 \approx 709.9 - 271.394 = 438.506$

Проверим эти результаты с использованием функций из библиотеки numpy.

```python
import numpy as np

# Данные
zp = np.array([35, 45, 190, 200, 40, 70, 54, 150, 120, 110])
ks = np.array([401, 574, 874, 919, 459, 739, 653, 902, 746, 832])

plt.scatter(x=zp, y=ks)

# Создаем и обучаем модель линейной регрессии с интерсептом
model = LinearRegression()
zp = zp.reshape(-1, 1)
model.fit(zp, ks)

r = model.score(zp, ks)
print(f"Коэффициент детерминации: {r}")

a = model.intercept_
b = float(model.coef_)
print(f"Коэффициенты линейной регрессии: {a}, {b}")
print(f"Итоговое уравнение регрессии: y = {a:0.2f} + {b:0.2f} * x")

plt.plot(zp, a + b * zp, color="red")
```

```
Коэффициент детерминации: 0.7876386635293685
Коэффициенты линейной регрессии: 444.1773573243595, 2.6205388824027662
Итоговое уравнение регрессии: y = 444.18 + 2.62 * x
```

Ручные расчеты: $\beta_0 \approx 438.51$;    $\beta_1 \approx 2.677$

Небольшое расхождение в значении $\beta_0$ может быть связано с округлением на различных этапах вычислений. Однако, в целом результаты довольно близки.

**Регрессия без интерсепта** ($\beta_0 = 0$)

Если нужно рассчитать линейную регрессию без интерсепта , то необходимо использовать только коэффициент наклона. Это можно сделать, исключив столбец единиц и вычислив только $\beta_1$:

```python

plt.scatter(x=zp, y=ks)

# Создаем и обучаем модель линейной регрессии без интерсепта
model = LinearRegression(fit_intercept=False)
model.fit(zp, ks)

r = model.score(zp, ks)
print(f"Коэффициент детерминации: {r}")

a = model.intercept_
b = float(model.coef_)
print(f"Коэффициенты линейной регрессии: {a}, {b}")
print(f"Итоговое уравнение регрессии: y = {b:0.2f} * x")

plt.plot(zp, a + b * zp, color="red")
```

```
Коэффициент детерминации: -0.8549037531632888
Коэффициенты линейной регрессии: 0.0, 5.889820420132689
Итоговое уравнение регрессии: y = 5.89 * x
```

Итоговые коэффициенты линейной регрессии

1. С интерсептом:
   $ks = 444.18 + 2.62 \cdot zp$

2. Без интерсепта:
   $ks = 5.89 \cdot zp$

- **С интерсептом:** Модель учитывает как фиксированный интерсепт, так и зависимость между заработной платой и кредитным скорингом.
- **Без интерсепта:** Модель предполагает, что при заработной плате, равной нулю, кредитный скоринг также будет равен нулю, что может быть менее реалистичным.

#### Задание 2

_Посчитать коэффициент линейной регрессии при заработной плате (zp), используя градиентный спуск (без intercept)._

Для вычисления коэффициента линейной регрессии при заработной плате ($zp$) с использованием градиентного спуска без интерсепта, нам нужно выполнить следующие шаги:

1. Определить функцию потерь (в данном случае, это среднеквадратичная ошибка).
   $L = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2$
   где $\hat{y}_i = \beta_1 x_i$.
2. Определить градиент функции потерь.
   $\frac{\partial L}{\partial \beta_1} = \frac{1}{m} \sum_{i=1}^{m} (\beta_1 x_i - y_i) x_i$
3. Итеративно обновлять коэффициенты, используя градиентный спуск.
   $\beta_1 := \beta_1 - \alpha \frac{\partial L}{\partial \beta_1}$
   где $\alpha$ — шаг обучения (learning rate).

```python
import numpy as np

# Данные
zp = np.array([35, 45, 190, 200, 40, 70, 54, 150, 120, 110])
ks = np.array([401, 574, 874, 919, 459, 739, 653, 902, 746, 832])

# Параметры градиентного спуска
alpha = 0.0001  # Шаг обучения
n_iterations = 10000  # Количество итераций
m = len(zp)  # Количество наблюдений

# Начальное значение коэффициента
beta_1 = 0

# Градиентный спуск
for _ in range(n_iterations):
    gradient = (1/m) * np.sum((beta_1 * zp - ks) * zp)
    beta_1 -= alpha * gradient

print(f"Коэффициент наклона линейной регрессии: {beta_1}")
print(f"Итоговое уравнение регрессии: y = {beta_1:0.2f} * x")
```

```
Коэффициент наклона линейной регрессии: 5.889820420132689
Итоговое уравнение регрессии: y = 5.89 * x
```

Этот результат совпадает с тем, который был получен с использованием функции линейной регрессии из библиотеки `scikit-learn` для модели без интерсепта. Это подтверждает корректность проведенных вычислений.

#### Задание 3

_Произвести вычисления как в пункте 2, но с вычислением intercept. Учесть, что изменение коэффициентов должно производиться на каждом шаге одновременно (то есть изменение одного коэффициента не должно влиять на изменение другого во время одной итерации)._

Для вычисления коэффициента линейной регрессии при заработной плате ($zp$) с использованием градиентного спуска без интерсепта, нам нужно выполнить следующие шаги:

1. Определить функцию потерь (в данном случае, это среднеквадратичная ошибка).
   $L = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2$
   где $\hat{y}_i = \beta_0 + \beta_1 x_i$.

2. Определить градиенты функции потерь.
   $\frac{\partial L}{\partial \beta_0} = \frac{1}{m} \sum_{i=1}^{m} (\beta_0 + \beta_1 x_i - y_i);\quad \frac{\partial L}{\partial \beta_1} = \frac{1}{m} \sum_{i=1}^{m} (\beta_0 + \beta_1 x_i - y_i) x_i$

3. Итеративно обновлять коэффициенты, используя градиентный спуск.
   $\beta_0 := \beta_0 - \alpha \frac{\partial L}{\partial \beta_0};\quad \beta_1 := \beta_1 - \alpha \frac{\partial L}{\partial \beta_1}$
   где $\alpha$ — шаг обучения (learning rate).

```python
import numpy as np

# Данные
zp = np.array([35, 45, 190, 200, 40, 70, 54, 150, 120, 110])
ks = np.array([401, 574, 874, 919, 459, 739, 653, 902, 746, 832])

# Параметры градиентного спуска
alpha = 0.0001  # Шаг обучения
n_iterations = 1000000  # Количество итераций
m = len(zp)  # Количество наблюдений

# Начальные значения коэффициентов
beta_0 = 0
beta_1 = 0

# Градиентный спуск
for _ in range(n_iterations):
    y_pred = beta_0 + beta_1 * zp
    error = y_pred - ks
    
    beta_0_gradient = (1/m) * np.sum(error)
    beta_1_gradient = (1/m) * np.sum(error * zp)
    
    beta_0 -= alpha * beta_0_gradient
    beta_1 -= alpha * beta_1_gradient

print(f"Коэффициенты линейной регрессии: {beta_0}, {beta_1}")
print(f"Итоговое уравнение регрессии: y = {beta_0:0.2f} + {beta_1:0.2f} * x")
```

```
Коэффициенты линейной регрессии: 444.177357319993, 2.6205388824349054
Итоговое уравнение регрессии: y = 444.18 + 2.62 * x
```

Результат вычислений с использованием градиентного спуска:

- Интерсепт ($\beta_0$): 444,18
- Коэффициент наклона ($\beta_1$): 2,62
- Итоговое уравнение регрессии: ks = 444,18 + 2,62 * zp

Чтобы добиться совпадения результатов с scikit-learn, градиентный спуск требует тонкой настройки параметров, таких как шаг обучения и количество итераций.
